# -*- coding: utf-8 -*-
"""del_lab4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vg9F68xR2fRrI9KQQPvQVbfJU87IT72O
"""

from sklearn.datasets import load_iris
import pandas as pd
from numpy import *

data = load_iris()
df = pd.DataFrame(data.data)

def leaky_relu(x):
  if x>0:
    return x
  else:
    return 0.01*x

def relu(x):
  if int(x) > 0:
    return int(x)
  else:
    return 0

w11 = matrix([0.5]*4) # input weights for h1
w12 = matrix([0.5]*4) # input weights for h2

h1 = matrix(df)*w11.T # hidden layer h1
h2 = matrix(df)*w12.T # hidden layer h2

# activation function
y1 = []
for i in h1:
  y1.append(leaky_relu(i))

y2= []
for i in h2:
  y2.append(leaky_relu(i))

w21 = [0.1]*2 # output layer weights
h3 = array(y1)*w21[0] + array(y2)*w21[1]

# final output
y_pred = []
for i in h3:
  y_pred.append(relu(i))

array(y_pred)

y_act = data.target

def softmax(v):
  e = exp(v)
  return e/sum(e)

from sklearn.datasets import load_iris

data = load_iris()
df = pd.DataFrame(data.data)
w1 = array([[0.5]*4]*3)
w2 = array([[0.1]*3]*3)

def calc(w1, w2):
  # hidden layer
  h1 = matrix(df)*w1.T
  y1 = tanh(h1) # hidden layer output

  # output layer
  h2 = matrix(y1)* w2.T
  y_pred = []
  for i in h2:
    y_pred.append(array([softmax(i)]))
  return y_pred
calc(w1,w2)

y_act = pd.DataFrame(data.target)
m = {0: array([1.0, 0.0, 0.0]), 1: array([0.0, 1.0, 0.0]), 2: array([0.0, 0.0, 1.0])}

# Use applymap with a lambda function to achieve the desired mapping
y_act = y_act.applymap(lambda x: m.get(x))
y_act

# https://medium.com/@anishnama20/understanding-cost-functions-in-machine-learning-types-and-applications-cd7d8cc4b47d#:~:text=Categorical%20Cross%2DEntropy%3A%20This%20is,of%20the%20multiple%20output%20classes.

y_pred = calc(w1,w2)
n = len(y_pred)
for i in range(100):
  #d = -1/n * sum(sum(matrix(y_act) * log(y_pred))) # cost function

  y_pred_array = array(y_pred)
  y_act_array = array(y_act.iloc[:,0].tolist()) # Get the first column of y_act as a list
  #d = -1/n * sum(y_act_array * log(y_pred_array))
  d = y_act_array - y_pred_array
  w1 = w1 - 0.001*d
  w2 = w2 - 0.001*d
  y_pred_array = array(calc(w1,w2))